BBM30
JB1571



a.2)
The drawbacks would be that multiplication is more computationally instensive than addition and
accuracy is lost in multiple successive multiplications.

a.5)
10% of data: 0.635
20% of data: 0.716
30% of data: 0.735
40% of data: 0.75
50% of data: 0.782
60% of data: 0.785
70% of data: 0.793
80% of data: 0.802
90% of data: 0.81
100% of data: 0.825

a.6)
We looked at the y=lnx curve and we noticed that if we set k=e, we would have all of our results
positive, and that's what we tried first, this would give us an output range of [0, ln(e+1)]. We then
thought that by moving the output range down to include negative numbers as well, so we tried to
divide e by different ingtegers and found that e/9 yields the maximum output when substituted in the
formulae that are provided in the link in the assignment description. That gave us 73% accuracy. This
would imply that 8/9th of the range would be negative numbers and 1/9th positive => low accuracy is a
big penalty and decreases your score. However, we noticed that by changing the formula a little bit by
kicking k outside of the log and adding a very small number to the probability to avoid log(0) we got
82.5% prediction accuracy. So we went with this second approach with k + log(p+n). where k = e, p =
probability that this feature is True and n = 0.0000001.
We think the report is reasonably good since > 80% leads to extra credit 

b.1)
Feature 1: Correct the shearing of the image. We draw a line that on average follows the 'lean' of the
image. This is done by calculating where the image begins and ends for each row and taking the
average of those and considering it as a line slope. Then the image is straightened by moving each row
towards the vertical line(center) x = 0. This allows each image to be oriented in the right direction and
it might lead to a better match since we see that some images like a lot of 1-s are tilted to the right.
Feature 2: Fill the pixel holes. We saw that some numbers had holes in their pixel form. Meaning there
were ' ' surrounded on all sides by '#' or '+'. We therefore made an algorithm to correct this and it would
lead to a more uniform stroke and the match was expected to be slightly higher.
Feature 3: Enlarge the image by a factor of 4. This feature worked exceptionally well by applying it
after the first feature and before the second feature. This made the image bigger and therefore the gaps
between strokes of a digit bigger. For example two gaps in the digit 8. And therefore we were hoping to
achieve a better accuracy by decreassing the blurryness of the close pars of the digit.
10% of data: 0.12620% of data: 0.225
30% of data: 0.104
40% of data: 0.094
50% of data: 0.188
60% of data: 0.089
70% of data: 0.352
80% of data: 0.638
90% of data: 0.751
100% of data: 0.826

b.2)
All of our features already work on top of the boolean representation of the initial image, so they all
fundamentally implement the basic feature extraction and therefore our numbers were the same for b.1
and b.2:

10% of data: 0.126
20% of data: 0.225
30% of data: 0.104
40% of data: 0.094
50% of data: 0.188
60% of data: 0.089
70% of data: 0.352
80% of data: 0.638
90% of data: 0.751
100% of data: 0.826
